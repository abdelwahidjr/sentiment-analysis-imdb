{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Sentiment Analysis of IMDB Movie Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Problem Statement:**\n",
    "\n",
    "In this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1424638f5259100af9f9a5c1b05bd23cf5b71e51",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./input\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4c593c17588723c0b0b0f19851cb70a8447ced76",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n5  Probably my all-time favorite movie, a story o...  positive\n6  I sure would like to see a resurrection of a u...  positive\n7  This show was an amazing, fresh & innovative i...  negative\n8  Encouraged by the positive comments about this...  negative\n9  If you like original gut wrenching laughter yo...  positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Probably my all-time favorite movie, a story o...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I sure would like to see a resurrection of a u...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This show was an amazing, fresh &amp; innovative i...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Encouraged by the positive comments about this...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>If you like original gut wrenching laughter yo...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('./IMDB-Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Import the training dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ad3773974351ed9bdf389b2847d7475b36c2295",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Exploratery data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7f11c83b1320c8982b36889145f7f770563674a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   review sentiment\ncount                                               50000     50000\nunique                                              49582         2\ntop     Loved today's show!!! It was a variety and not...  negative\nfreq                                                    5     25000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50000</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49582</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Loved today's show!!! It was a variety and not...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>25000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset\n",
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "453c3fd238f62ab8f649eb01771817e25bc0c77d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Sentiment count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cb6bb97b0f851947dcf341a1de5708a1f2bc64c1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "negative    25000\npositive    25000\nName: sentiment, dtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment count\n",
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f61964573faababe1f7897b77d32815a24954d2f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Spliting the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d3aaabff555e07feb11c72cc3a6e457615975ffe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "#test dataset\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90da29c3b79f46f41d7391a2a116065b616d0fac",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Text normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f000c43d91f68f6668539f089c6a54c5ce3bd819",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "328b6e5977da3e055ad4b2e11a31e5e12ccf3b16",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Removing html strips and noise text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f6fcafbdadcdcb0c164e37d71fb9d1623f74d0a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88117b74761d1047924d6d70f76642faa0e706ac",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Removing special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "219da72b025121fd98081df50ae0fcaace10cc9d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b66eeabd5b7b8c251f8b8ddf331140a64bcd514",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Text stemming\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2295f2946e0ab74c220ad538d0e7adc04d23f697",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e83107e4a281d84d7ae42b4e2c8d81b7ece438e4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Removing stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5dbff82b4d2d188d8777b273a75d8ac714d38885",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b35e7499291173119ed42287deac6f0cd96516e1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Normalized train reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b20c242bd091929ca896ea2c6e936ca00efe6ecf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#normalized train reviews\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_train_reviews[0]\n",
    "#convert dataframe to string\n",
    "#norm_train_string=norm_train_reviews.to_string()\n",
    "#Spelling correction using Textblob\n",
    "#norm_train_spelling=TextBlob(norm_train_string)\n",
    "#norm_train_spelling.correct()\n",
    "#Tokenization using Textblob\n",
    "#norm_train_words=norm_train_spelling.words\n",
    "#norm_train_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d69462bb209a66cff86376dc8481d0c0140d894d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Normalized test reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "c5d0d38bd9976150367e9d75f3b933774c96a1ab",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Normalized test reviews\n",
    "norm_test_reviews=imdb_data.review[40000:]\n",
    "norm_test_reviews[45005]\n",
    "##convert dataframe to string\n",
    "#norm_test_string=norm_test_reviews.to_string()\n",
    "#spelling correction using Textblob\n",
    "#norm_test_spelling=TextBlob(norm_test_string)\n",
    "#print(norm_test_spelling.correct())\n",
    "#Tokenization using Textblob\n",
    "#norm_test_words=norm_test_spelling.words\n",
    "#norm_test_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c2a872ffcb6b8076fdbbba641af12081b6022ef",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bags of words model **\n",
    "\n",
    "It is used to convert text documents to numerical vectors or bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35cf9dcefb40b2dc520c5b0d559695324c46cc04",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Term Frequency-Inverse Document Frequency model (TFIDF)**\n",
    "\n",
    "It is used to convert text documents to  matrix of  tfidf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afe6de957339921e05a6faeaf731f2272fd31946",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "803e92b25faa738b10928a91de72d177d8dddf85",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Labeling the sentiment text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60f5d496ce4109d1cdbf08f4284d4d26efd93922",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21a80c94fb42e14391c627710c5d796c40aa7dde",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Split the sentiment tdata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "ca1e4cc917265ac98a72c37cffe57f27e9897408",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Modelling the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5e45fdc9d062a5b9b9dd665ffe732776e196953",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us build logistic regression model for both bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "142d007421900550079a12ae8655bcae678ebaad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07eb6d52eb32469e3be82e90af636d598a7b7c27",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Logistic regression model performane on test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52ad86935b76117f97b79e6672a3ba12352b9461",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
    "print(\"lr_bow_score :\",lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac2ec8353acb5e0f548e1e4a590fbe6f34f4a686",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Print the classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f89c7e7a6136d08790ffbf6bc4d0d05455f8555a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(lr_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d2e5ddcd69ff0fb52f05f17fc74a86e1b5e5b61",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a36c058e834938559b7202f2142e61423a613b7a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fde9753386e3593dc27c4e88e02bdc38462a018",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Stochastic gradient descent or Linear support vector machines for bag of words and tfidf features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2211a9e97682195a0372b33e4da7267aad8548db",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#training the linear svm\n",
    "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
    "#fitting the svm for bag of words\n",
    "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
    "print(svm_bow)\n",
    "#fitting the svm for tfidf features\n",
    "svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9a7a973591c1d3cabaa1a47c57fa029d3752bab",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Model performance on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a5ab738e04f0f9082c8d6ade6c2148cc398f8f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "svm_bow_predict=svm.predict(cv_test_reviews)\n",
    "print(svm_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "svm_tfidf_predict=svm.predict(tv_test_reviews)\n",
    "print(svm_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "svm_bow_score=accuracy_score(test_sentiments,svm_bow_predict)\n",
    "print(\"svm_bow_score :\",svm_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "svm_tfidf_score=accuracy_score(test_sentiments,svm_tfidf_predict)\n",
    "print(\"svm_tfidf_score :\",svm_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1bd245f50902ad87ca28e48cbce64ec6a16ec5a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Print the classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d112bc5b4944330b567e19a7e04544a9a459f238",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "svm_bow_report=classification_report(test_sentiments,svm_bow_predict,target_names=['Positive','Negative'])\n",
    "print(svm_bow_report)\n",
    "#Classification report for tfidf features\n",
    "svm_tfidf_report=classification_report(test_sentiments,svm_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(svm_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "705fd8ae8bb5e6925852fffc906b6ffd769dbac0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Plot the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49cde912705acbaef90d7a269cd27ea8a2815f03",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,svm_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,svm_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Multinomial Naive Bayes for bag of words and tfidf features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
    "print(mnb_bow)\n",
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
    "print(mnb_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Model performance on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Print the classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)\n",
    "#Classification report for tfidf features\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Plot the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Let us see positive and negative words by using WordCloud.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Word cloud for positive review words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#word cloud for positive review words\n",
    "plt.figure(figsize=(10,10))\n",
    "positive_text=norm_train_reviews[1]\n",
    "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "positive_words=WC.generate(positive_text)\n",
    "plt.imshow(positive_words,interpolation='bilinear')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Word cloud for negative review words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Word cloud for negative review words\n",
    "plt.figure(figsize=(10,10))\n",
    "negative_text=norm_train_reviews[8]\n",
    "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "negative_words=WC.generate(negative_text)\n",
    "plt.imshow(negative_words,interpolation='bilinear')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Conclusion:**\n",
    "* We can observed that both logistic regression and multinomial naive bayes model performing well compared to linear support vector  machines.\n",
    "* Still we can improve the accuracy of the models by preprocessing data and by using lexicon models like Textblob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}